{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Web Scraping\n",
    "\n",
    "To begin, we will examine the reddit page dealing with Machine Learning.  Our goal is to scrape the basic information for posts.\n",
    "\n",
    "![](images/reddit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.reddit.com/r/MachineLearning/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<!doctype html>\\n<html>\\n  <head>\\n    <title>Too Many Requests</title>\\n    <style>\\n      body {\\n          font: small verdana, arial, helvetica, sans-serif;\\n          width: 600px;\\n          margin: 0 auto;\\n      }\\n\\n      h1 {\\n          height: 40px;\\n          background: transparent url(//www.redditstatic.com/reddit.com.header.png) no-repeat scroll top right;\\n      }\\n    </style>\\n  </head>\\n  <body>\\n    <h1>whoa there, pardner!</h1>\\n    \\n\\n\\n<p>we\\'re sorry, but you appear to be a bot and we\\'ve seen too many requests\\nfrom you lately. we enforce a hard speed limit on requests that appear to come\\nfrom bots to prevent abuse.</p>\\n\\n<p>if you are not a bot but are spoofing one via your browser\\'s user agent\\nstring: please change your user agent string to avoid seeing this message\\nagain.</p>\\n\\n<p>please wait 6 second(s) and try again.</p>\\n\\n    <p>as a reminder to developers, we recommend that clients make no\\n    more than <a href=\"http://github.com/reddit/reddit/wiki/API\">one\\n    request every two seconds</a> to avoid seeing this message.</p>\\n  </body>\\n</html>\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [429]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitchfork and Webscraping\n",
    "\n",
    "This is an introduction to our work from class.  You are asked to further this for your third lab assignment.\n",
    "\n",
    "### Webpages and HTML\n",
    "\n",
    "The body of a webpage consists of many HTML tags.  HTML itself is a markup language, that surrounds text with \"tags\" -- a start tag `<>` and end tag `</>` that summarily apply formatting.  For example\n",
    "\n",
    "```HTML\n",
    "<h1>This is a Header</h1>\n",
    "```\n",
    "\n",
    "makes a header.  Different tags have different functions, and we will use these tags to navigate content we are interested in on a webpage.  We can use HTML magic in a Jupyter notebook to play with rendering HTML ourselves.  The code below demonstrates tags (`h1`), tags attributes (`class= 'super-paragraph')` and tags within tags (`<p><strong></strong></p>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>This is a header</h1>\n",
       "<p class = 'super-paragraph'>This would be a paragraph. <strong>Strong Words</strong> here. </p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h1>This is a header</h1>\n",
    "<p class = 'super-paragraph'>This would be a paragraph. <strong>Strong Words</strong> here. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to graph some basic information about reviews from pitchfork.  To begin, let's examine the page at Pitchfork housing the reviews.  By the look of the page, we see that there is a consistent structure and layout that we can exploit.  \n",
    "\n",
    "![](images/pitch1.png)\n",
    "\n",
    "We want to start by scraping the six elements available for each review:\n",
    "\n",
    "- image\n",
    "- artist\n",
    "- album\n",
    "- genre \n",
    "- review\n",
    "- date\n",
    "\n",
    "First, we will import our standard libraries in addition to `requests` and `bs4`.  The requests library will get us the code for webpages, while BeautifulSoup will help us parse this content and extract the information that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save url as variable\n",
    "url = 'https://pitchfork.com/reviews/albums/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save page as response after using requests to get the code\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass the text of the page to BeautifulSoup object\n",
    "#and save this as soup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigating the Tree\n",
    "\n",
    "If we right click on the title of the album, we can use the inspect tool look at the HTML for this element.  When we do, we see the following:\n",
    "\n",
    "![](images/pitch2.png)\n",
    "\n",
    "It looks like every review is contained in a tag that looks like:\n",
    "\n",
    "```HTML\n",
    "<div class = \"review\">\n",
    "```\n",
    "\n",
    "Accordingly, we can use the `.find` method to search for this.  Here, we look for the `div` tag, and for div tags that also contain `class = \"review\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"review\"><a class=\"review__link\" href=\"/reviews/albums/brockhampton-iridescence/\"><div class=\"review__artwork artwork\"><div class=\"\"><img alt=\"Cover of Iridescence \" src=\"https://media.pitchfork.com/photos/5ba8f50266ff630650f8e4ea/1:1/w_160/brockhampton_iridescence.jpg\"/></div></div><div class=\"review__title\"><ul class=\"artist-list review__title-artist\"><li>BROCKHAMPTON</li></ul><h2 class=\"review__title-album\">iridescence</h2></div></a><div class=\"review__meta\"><ul class=\"genre-list genre-list--inline review__genre-list\"><li class=\"genre-list__item\"><a class=\"genre-list__link\" href=\"/reviews/albums/?genre=rap\">Rap</a></li></ul><ul class=\"authors\"><li><a class=\"linked display-name display-name--linked\" href=\"/staff/reed-jackson/\"><span class=\"by\">by: </span>Reed Jackson</a></li></ul><time class=\"pub-date\" datetime=\"2018-09-26T05:00:00\" title=\"Wed, 26 Sep 2018 05:00:00 GMT\">21 hrs ago</time></div></div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('div', {'class': 'review'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the basic information about the first review is here.  Now, if we want, we can search within our search results.  To do so, we will save the results as our first review and search the remaining elements using this as a starting place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_1 = soup.find('div', {'class': 'review'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, note that our name of the artist is inside a `li` tag, the album in an `h2` tag, genre in `li` tags, and so on.  The code below demonstrates finding each element.\n",
    "\n",
    "![](images/pitch2b.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li>BROCKHAMPTON</li>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_1.find('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BROCKHAMPTON'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_1.find('li').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iridescence'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_1.find('h2').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rap'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_1.find('li',{'class': 'genre-list__item'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'by: Reed Jackson'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_1.find('ul', {'class': 'authors'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'21 hrs ago'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_1.find('time').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Scraping Multiple Reviews\n",
    "\n",
    "Now, we will grab everything we can get off the first page.  This will be a list of elements like the one we just navigated, containing multiple albums information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = soup.find_all('div', {'class': 'review'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many reviews?\n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"review\"><a class=\"review__link\" href=\"/reviews/albums/brockhampton-iridescence/\"><div class=\"review__artwork artwork\"><div class=\"\"><img alt=\"Cover of Iridescence \" src=\"https://media.pitchfork.com/photos/5ba8f50266ff630650f8e4ea/1:1/w_160/brockhampton_iridescence.jpg\"/></div></div><div class=\"review__title\"><ul class=\"artist-list review__title-artist\"><li>BROCKHAMPTON</li></ul><h2 class=\"review__title-album\">iridescence</h2></div></a><div class=\"review__meta\"><ul class=\"genre-list genre-list--inline review__genre-list\"><li class=\"genre-list__item\"><a class=\"genre-list__link\" href=\"/reviews/albums/?genre=rap\">Rap</a></li></ul><ul class=\"authors\"><li><a class=\"linked display-name display-name--linked\" href=\"/staff/reed-jackson/\"><span class=\"by\">by: </span>Reed Jackson</a></li></ul><time class=\"pub-date\" datetime=\"2018-09-26T05:00:00\" title=\"Wed, 26 Sep 2018 05:00:00 GMT\">21 hrs ago</time></div></div>,\n",
       " <div class=\"review\"><a class=\"review__link\" href=\"/reviews/albums/richard-swift-the-hex/\"><div class=\"review__artwork artwork\"><div class=\"\"><img alt=\"Album cover of the Hex by Richard Swift\" src=\"https://media.pitchfork.com/photos/5ba40f260d9f560d50d5478e/1:1/w_160/Richard%20Swift_the%20Hex.jpg\"/></div></div><div class=\"review__title\"><ul class=\"artist-list review__title-artist\"><li>Richard Swift</li></ul><h2 class=\"review__title-album\">The Hex</h2></div></a><div class=\"review__meta\"><ul class=\"genre-list genre-list--inline review__genre-list\"><li class=\"genre-list__item\"><a class=\"genre-list__link\" href=\"/reviews/albums/?genre=rock\">Rock</a></li></ul><ul class=\"authors\"><li><a class=\"linked display-name display-name--linked\" href=\"/staff/sam-sodomsky/\"><span class=\"by\">by: </span>Sam Sodomsky</a></li></ul><time class=\"pub-date\" datetime=\"2018-09-26T05:00:00\" title=\"Wed, 26 Sep 2018 05:00:00 GMT\">21 hrs ago</time></div></div>,\n",
       " <div class=\"review\"><a class=\"review__link\" href=\"/reviews/albums/bhad-bhabie-15/\"><div class=\"review__artwork artwork\"><div class=\"\"><img alt=\"Album cover of 15 by Bhad Bhabie\" src=\"https://media.pitchfork.com/photos/5ba255c397e58c2d5455ed96/1:1/w_160/15_bhad%20bhabie.jpg\"/></div></div><div class=\"review__title\"><ul class=\"artist-list review__title-artist\"><li>Bhad Bhabie</li></ul><h2 class=\"review__title-album\">15</h2></div></a><div class=\"review__meta\"><ul class=\"genre-list genre-list--inline review__genre-list\"><li class=\"genre-list__item\"><a class=\"genre-list__link\" href=\"/reviews/albums/?genre=rap\">Rap</a></li></ul><ul class=\"authors\"><li><a class=\"linked display-name display-name--linked\" href=\"/staff/michelle-kim/\"><span class=\"by\">by: </span>Michelle Kim</a></li></ul><time class=\"pub-date\" datetime=\"2018-09-26T05:00:00\" title=\"Wed, 26 Sep 2018 05:00:00 GMT\">21 hrs ago</time></div></div>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first three reviews\n",
    "reviews[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `for` loops\n",
    "\n",
    "Now, we need a way to got through and rip the information out of these systematically -- enter the `for` loop.  Take the sample list below; `l`.  If we wanted to go inside this list, and do something to each element, we can use a `for` loop.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['steve', 'bobo', 1, 'u2', 'charles bradley']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steve\n",
      "bobo\n",
      "1\n",
      "u2\n",
      "charles bradley\n"
     ]
    }
   ],
   "source": [
    "for entry in l:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we can name the `entry` piece anything we like.  It should make sense, I have a bad habit of naming everything i ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "for ent in l:\n",
    "    print(type(ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Howdy, this time through, we're on  steve congratulations.\n",
      "Howdy, this time through, we're on  bobo congratulations.\n",
      "Howdy, this time through, we're on  1 congratulations.\n",
      "Howdy, this time through, we're on  u2 congratulations.\n",
      "Howdy, this time through, we're on  charles bradley congratulations.\n"
     ]
    }
   ],
   "source": [
    "for enty in l:\n",
    "    print(\"Howdy, this time through, we're on \", enty, \"congratulations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For us, we will create new lists from the existing list of reviews.  Each list will contain the elements of the album reviews, and we will loop through the reviews variable, appending each element to their respective lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = []\n",
    "albums = []\n",
    "genre = []\n",
    "author = []\n",
    "when = []\n",
    "for review in reviews:\n",
    "    t = review.find('li').text\n",
    "    artists.append(t)\n",
    "    s = review.find('h2').text\n",
    "    albums.append(s)\n",
    "    genre.append(review.find('li',{'class': 'genre-list__item'}).text)\n",
    "    author.append(review.find('ul', {'class': 'authors'}).text)\n",
    "    when.append(review.find('time').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.DataFrame({'artist': artists, 'albums': albums, 'genre': genre, 'author': author, 'when': when})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>albums</th>\n",
       "      <th>artist</th>\n",
       "      <th>author</th>\n",
       "      <th>genre</th>\n",
       "      <th>when</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iridescence</td>\n",
       "      <td>BROCKHAMPTON</td>\n",
       "      <td>by: Reed Jackson</td>\n",
       "      <td>Rap</td>\n",
       "      <td>21 hrs ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Hex</td>\n",
       "      <td>Richard Swift</td>\n",
       "      <td>by: Sam Sodomsky</td>\n",
       "      <td>Rock</td>\n",
       "      <td>21 hrs ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>Bhad Bhabie</td>\n",
       "      <td>by: Michelle Kim</td>\n",
       "      <td>Rap</td>\n",
       "      <td>21 hrs ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quiet River of Dust Vol. 1</td>\n",
       "      <td>Richard Reed Parry</td>\n",
       "      <td>by: Stuart Berman</td>\n",
       "      <td>Experimental</td>\n",
       "      <td>21 hrs ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Piano &amp; a Microphone 1983</td>\n",
       "      <td>Prince</td>\n",
       "      <td>by: Chris Randle</td>\n",
       "      <td>Pop/R&amp;B</td>\n",
       "      <td>September 25 2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       albums              artist             author  \\\n",
       "0                 iridescence        BROCKHAMPTON   by: Reed Jackson   \n",
       "1                     The Hex       Richard Swift   by: Sam Sodomsky   \n",
       "2                          15         Bhad Bhabie   by: Michelle Kim   \n",
       "3  Quiet River of Dust Vol. 1  Richard Reed Parry  by: Stuart Berman   \n",
       "4   Piano & a Microphone 1983              Prince   by: Chris Randle   \n",
       "\n",
       "          genre               when  \n",
       "0           Rap         21 hrs ago  \n",
       "1          Rock         21 hrs ago  \n",
       "2           Rap         21 hrs ago  \n",
       "3  Experimental         21 hrs ago  \n",
       "4       Pop/R&B  September 25 2018  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping a Wikipedia Table\n",
    "\n",
    "In this example, we will get information from a table containing characteristics about 21 jump street. \n",
    "\n",
    "![](images/jumpst.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_21_Jump_Street_episodes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\"/>\\n<title>List of 21 Jump Street episodes - Wikipedia</title>\\n<script>document.documentElement.className = document.documentElement.className.replace( /(^|\\\\s)client-nojs(\\\\s|$)/, \"$1client-js$2\" );</script>\\n<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"List_of_21_Jump_Street_episodes\",\"wgTitle\":\"List of 21 Jump Street episodes\",\"wgCurRevisionId\":844038329,\"wgRevisionId\":844038329,\"wgArticleId\":35403829,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Articles needing additional references from May 2012\",\"All articles needing additional references\",\"21 Jump Street\",\"Lists of American crime television series episodes\"],\"wgBreakFrames\":false,\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgSeparatorTransfo'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h2>Contents</h2>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_h2 = soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season 1 (1987)[edit]\n",
      "Season 2 (1987-88)[edit]\n",
      "Season 3 (1988-89)[edit]\n",
      "Season 4 (1989-90)[edit]\n",
      "Season 5 (1990-91)[edit]\n"
     ]
    }
   ],
   "source": [
    "for header in all_h2[2:7]:\n",
    "    print(header.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_1 = soup.find('table', {'class': 'wikitable plainrowheaders'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_1_titles = table_1.find_all('td', {'class': 'summary'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Pilot\"\n",
      "\"America, What a Town\"\n",
      "\"Don't Pet the Teacher\"\n",
      "\"My Future's So Bright, I Gotta Wear Shades\"\n",
      "\"The Worst Night of Your Life\"\n",
      "\"Gotta Finish the Riff\"\n",
      "\"Bad Influence\"\n",
      "\"Blindsided\"\n",
      "\"Next Generation\"\n",
      "\"Low and Away\"\"Running on Ice\"\n",
      "\"16 Blown to 35\"\n",
      "\"Mean Streets and Pastel Houses\"\n"
     ]
    }
   ],
   "source": [
    "for title in season_1_titles:\n",
    "    print(title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p><i><a href=\"/wiki/21_Jump_Street\" title=\"21 Jump Street\">21 Jump Street</a></i> is an American <a href=\"/wiki/Police_procedural\" title=\"Police procedural\">police procedural</a> <a class=\"mw-redirect\" href=\"/wiki/Crime_drama\" title=\"Crime drama\">crime drama</a> <a class=\"mw-redirect\" href=\"/wiki/Television_series\" title=\"Television series\">television series</a> that aired on the <a href=\"/wiki/Fox_Broadcasting_Company\" title=\"Fox Broadcasting Company\">Fox Network</a> and in first run syndication from April 12, 1987, to April 27, 1991, with a total of 103 <a href=\"/wiki/Episode\" title=\"Episode\">episodes</a>. The series focuses on a squad of youthful-looking undercover police officers investigating crimes in high schools, colleges, and other teenage venues.<sup class=\"reference\" id=\"cite_ref-1\"><a href=\"#cite_note-1\">[1]</a></sup>\n",
       "</p>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href = 'https://www.reddit.com/r/MachineLearning/'> The Reddit Page </a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<a href = 'https://www.reddit.com/r/MachineLearning/'> The Reddit Page </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all('h2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('a', {'data-click-id': 'body'})['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "for i in soup.find_all('a', {'data-click-id': 'body'}):\n",
    "    url_link = 'https://www.reddit.com' + i['href']\n",
    "    links.append(url_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "titles = []\n",
    "bodys = []\n",
    "for i in soup.find_all('a', {'data-click-id': 'body'}):\n",
    "    url_link = 'https://www.reddit.com' + i['href']\n",
    "    links.append(url_link)\n",
    "    response = requests.get(url_link)\n",
    "    soup2 = BeautifulSoup(response.text, 'html.parser')\n",
    "    title = soup2.find('h2')\n",
    "    body = soup2.find_all('p')\n",
    "    titles.append(title)\n",
    "    bodys.append(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'links': links, 'title': titles, 'body': bodys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia Exercise\n",
    "\n",
    "Scraping Wikipedia tables and adding information found through links.\n",
    "\n",
    "![](images/wiki_table.png)\n",
    "\n",
    "Problem:\n",
    "\n",
    "1. Create a dataframe that contains the information displayed on the Wikipedia page \"List of 2018 Albums\".\n",
    "2. What is Sub Pop releasing in 2018?\n",
    "3. Did Drake put anything out?\n",
    "4. What label is putting out the most music?  Visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_2018_albums'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('table', {'class':'wikitable'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "consumer_key = 'o24LbkkTsV3eVKERVYjIznnrT'\n",
    "\n",
    "consumer_secret = 'Q4yUOhDhlagNWrgwOnqzroGHI5aWqaM1MkbkkO6p9gPRhtKIYz'\n",
    "access_token = '820718295187918848-DjESel4eJhmWto48EwBrmkCBR5vthkZ'\n",
    "access_token_secret = 'fC8KyuUJoPOft2hIpCvNVf4dWj2FH5zw6IMgdcIbqNmCK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweepy\n",
    "\n",
    "- Sign into Twitter apps (https://apps.twitter.com/)\n",
    "- Create application and retrieve `consumer_key`, `consumer_secret`, `access_token`, and `access_token_secret`.  \n",
    "- Follow example below filling in your info.  For more info, see the Tweepy documentation [here](http://tweepy.readthedocs.io/en/v3.5.0/getting_started.html#introduction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = api.get_user('thrashermag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the pigs feet and dog coats didn't raise the red flag, the $600 in pizza definitely did the trick. No wonder theâ€¦ https://t.co/juUgD90o4m\n",
      "Style and brains eternal, we love you Phil Shao!  https://t.co/x6osDOcDFO https://t.co/BumbRgl2Kx\n",
      "The WKND crew took a U-Haul full of ramps to the desert and things got weird. They may not have located Animal Chinâ€¦ https://t.co/PqZ6jpe26p\n",
      "With a nod to Jeremy Klein, the WKND boys hit the road Hook Ups style and put together a launch-ramp-infused tour,â€¦ https://t.co/9Sw1m5TJum\n",
      "After smashing his grill, Jaws rejoins Real for a heavy handrail day with Jamie Thomas. Tyson goes for the biggestâ€¦ https://t.co/nsuPXrr7lr\n",
      "If Dustin Dollin is signing your checks you sure as hell arenâ€™t gonna turn in any soft footage. This PD Promo is haâ€¦ https://t.co/qTl5xpyBpm\n",
      "Randy Blythe looks out for his people, loves what he does and just wants his fans to know where heâ€™s coming from.â€¦ https://t.co/gHaVT50zEX\n",
      "Frog in Las Vegas, Gridlock in SF, Brent Atchley's return and more in today's episode of Skateline.â€¦ https://t.co/4BjpB74d0P\n",
      "Jamie Thomas jumps in the Real van to coach them through a day of big-ass rails and \"one more\" tries. Watch the fulâ€¦ https://t.co/jYIKFh4S5r\n",
      "No soundtrack, just the sights and sounds of two comrades creating majestic moments on their skateboards.â€¦ https://t.co/ggyNi3w1bV\n",
      "Mike Arnold ðŸŒŠ Atlantic Drift https://t.co/jLNPbYrmnC\n",
      "It's the up-grinding powers of Cole Wilson vs Zion's unstoppable ATV skills this week. Oh, and Evan Smith's unorthoâ€¦ https://t.co/iMx9axcNWS\n",
      "After trips to SF and Vegas, the Drifters elected to test their mettle atop the volcanic peaks of paradise, bringinâ€¦ https://t.co/wiH331wNbI\n",
      "SF is the promised land. Come see for yourself. Everything is better when fried...\n",
      "https://t.co/vulmaPqRac https://t.co/HXIQYQUiog\n",
      "Realâ€™s rusty nail isnâ€™t just tech tricks and precision ledgework, heâ€™s down to get weird AF! Could a KOTR win (andâ€¦ https://t.co/hcEG5F0Sb0\n",
      "All the coffins, footballs and diapers slams â€“ without all that fancy talkinâ€™ and TV stuff. Bam gets bent, Brock geâ€¦ https://t.co/pNjZ3SkygP\n",
      "The Skate Gods deployed the Skate Rock crew to Vietnam and Bali. They stay ready for anything that comes their way.â€¦ https://t.co/buTsITIl4u\n",
      "Rowan and Frecks kick it off with a rip ride around Vietnam, meeting Ishod and the boys for an island hop to Bali dâ€¦ https://t.co/QIpnVM1f5h\n",
      "Big head, bigger pop! Masonâ€™s gonna crush KOTR, unless that hockey temper takes over. https://t.co/gkkhnlKRnc https://t.co/rNCSIJnZuG\n",
      "A full-length extravaganza from the Isles of Hawaiiâ€¦ Thank you, APB! https://t.co/U7tuIidtTx https://t.co/cFxWP2kv0Q\n"
     ]
    }
   ],
   "source": [
    "for tweet in user.timeline(limit = 500):\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418430\n"
     ]
    }
   ],
   "source": [
    "print(user.followers_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for tweet in user.timeline(count = 200):\n",
    "    tweets.append(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"If the pigs feet and dog coats didn't raise the red flag, the $600 in pizza definitely did the trick. No wonder theâ€¦ https://t.co/juUgD90o4m\",\n",
       " 'Style and brains eternal, we love you Phil Shao!  https://t.co/x6osDOcDFO https://t.co/BumbRgl2Kx',\n",
       " 'The WKND crew took a U-Haul full of ramps to the desert and things got weird. They may not have located Animal Chinâ€¦ https://t.co/PqZ6jpe26p',\n",
       " 'With a nod to Jeremy Klein, the WKND boys hit the road Hook Ups style and put together a launch-ramp-infused tour,â€¦ https://t.co/9Sw1m5TJum',\n",
       " 'After smashing his grill, Jaws rejoins Real for a heavy handrail day with Jamie Thomas. Tyson goes for the biggestâ€¦ https://t.co/nsuPXrr7lr']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Table\n",
    "\n",
    "![](images/open_table.png)\n",
    "\n",
    "Finding restaurants in New York City. (https://www.opentable.com/new-york-restaurant-listings)  Is there good Indian food in the Upper West Side?  Where?  What are people saying is good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'https://www.opentable.com/new-york-restaurant-listings'\n",
    "response = requests.get('https://www.yelp.com/search?find_desc=burrito&find_loc=Civic+Center%2C+Manhattan%2C+NY&ns=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Response' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-37acf3ef279d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Response' object has no attribute 'view'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n  \\n\\n            window.yPageStart = new Date().getTime();\\n\\n            var initialVisibilityState = document.webkitVisibilityState;\\n\\n                yPerfTimings = [];\\n\\n                ySitRepParams = {\"clientIP\": \"144.121.201.14\", \"datacenter\": \"us-east-1\", \"is_internal_ip\": false, \"edgeStartT'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.text[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = soup.find_all('div', {'class': 'media-block media-block--18'})\n",
    "test2 = soup.find_all('a', {'class': 'biz-name'})\n",
    "title = []\n",
    "for t in test2:\n",
    "    title.append(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jerusalem Mexican Deli Grocery',\n",
       " 'El Vez',\n",
       " 'Holi Mole',\n",
       " 'Breakroom',\n",
       " 'Burrito House',\n",
       " 'Pulqueria',\n",
       " 'Dos Toros Taqueria',\n",
       " 'Habana To-Go',\n",
       " 'New Fresco Tortillas',\n",
       " 'Oaxaca Taqueria',\n",
       " 'Luchadores']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = soup.find_all('div', {'class': 'rest-row-header'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dorcass  \n",
      " Gregg Hegmann  \n",
      " Angelinas  \n",
      " Will  \n",
      " Lemke Ports  \n",
      " Herzog  \n",
      " Sed Erdman  \n",
      " Dolore  \n",
      " Et VonRueden  \n",
      " Rerum  \n",
      " Margarett Grant  \n",
      " Columbuss  \n",
      " 45 Kirlin  \n",
      " Mews  \n",
      " Saepe Stracke  \n",
      " Recusandae Quigley  \n",
      " Noemi Glover  \n",
      " Kemmer Ports  \n",
      " 1051 Dickinson  \n",
      " Liza Murazik  \n",
      " Earum Jacobson  \n",
      " 559 Hammes  \n",
      " Sunt Wiegand  \n",
      " Delectus  \n",
      " Jacksons  \n",
      " Brants  \n",
      " Simonis  \n",
      " Titus Cremin  \n",
      " Brenden Mills  \n",
      " Pollich  \n",
      " Columbus Pfeffer  \n",
      " Mason Pike  \n",
      " 1376 Prohaska  \n",
      " Branch  \n",
      " Tempore  \n",
      " Trail  \n",
      " Et  \n",
      " Autem  \n",
      " Hermiston  \n",
      " Crest  \n",
      " Quis Marks  \n",
      " Flo Crossroad  \n",
      " Consequatur Schinner  \n",
      " Raynor  \n",
      " Damariss  \n",
      " Agloe Bar & Grill  \n",
      " Forges  \n",
      " Marcia Shoal  \n",
      " Heathcote  \n",
      " Eum Tunnel  \n",
      " Arvilla Bosco  \n",
      " Jayde Key  \n",
      " Kunze  \n",
      " Mollie Heller  \n",
      " Exercitationem Summit  \n",
      " Lindsay Reichel  \n",
      " Quasi River  \n",
      " Crawford Willms  \n",
      " Similique  \n",
      " Luciano Hansen  \n",
      " Ratione Villages  \n",
      " Distinctio Ports  \n",
      " 1023 MacGyver  \n",
      " Ex Harbors  \n",
      " Nulla  \n",
      " Rerum Mews  \n",
      " Quasi  \n",
      " Natus Torphy  \n",
      " Beulahs  \n",
      " Groves  \n",
      " Facere Waelchi  \n",
      " Enim  \n",
      " Facere  \n",
      " Ryan  \n",
      " Eras  \n",
      " Goldner  \n",
      " Ebert Port  \n",
      " Dicta Trace  \n",
      " Facilis  \n",
      " Smitham Via  \n",
      " Hallies  \n",
      " Fidel Corner  \n",
      " Deserunt Kihn  \n",
      " Reanna Wolff  \n",
      " Sit  \n",
      " Colt Hansen  \n",
      " Quibusdam Hegmann  \n",
      " Voluptas Crescent  \n",
      " Soluta Sauer  \n",
      " Jolies  \n",
      " Fall  \n",
      " Howell  \n",
      " Kari Paucek  \n",
      " Voluptatem Koch  \n",
      " Denesik  \n",
      " Bradtke Throughway  \n",
      " 1061 Stehr  \n",
      " Gardens  \n",
      " Armstrong  \n",
      " Dominic Klein  \n"
     ]
    }
   ],
   "source": [
    "for name in names:\n",
    "    print(name.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
